%
% File acl2015.tex
%
% Contact: car@ir.hit.edu.cn, gdzhou@suda.edu.cn
%%
%% Based on the style files for ACL-2014, which were, in turn,
%% Based on the style files for ACL-2013, which were, in turn,
%% Based on the style files for ACL-2012, which were, in turn,
%% based on the style files for ACL-2011, which were, in turn, 
%% based on the style files for ACL-2010, which were, in turn, 
%% based on the style files for ACL-IJCNLP-2009, which were, in turn,
%% based on the style files for EACL-2009 and IJCNLP-2008...

%% Based on the style files for EACL 2006 by 
%%e.agirre@ehu.es or Sergi.Balari@uab.es
%% and that of ACL 08 by Joakim Nivre and Noah Smith

\documentclass[11pt]{article}
\usepackage{acl2015}
\usepackage{times}
\usepackage{url}
\usepackage{latexsym}
\usepackage{hyperref}
\usepackage{listings}
\usepackage{color}
\usepackage{graphicx}

\definecolor{dkgreen}{rgb}{0,0.6,0}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{mauve}{rgb}{0.58,0,0.82}

\lstset{frame=tb,
  language=bash,
  aboveskip=3mm,
  belowskip=3mm,
  showstringspaces=false,
  columns=flexible,
  basicstyle={\small\ttfamily},
  numbers=none,
  numberstyle=\tiny\color{gray},
  keywordstyle=\color{blue},
  commentstyle=\color{dkgreen},
  stringstyle=\color{mauve},
  breaklines=true,
  breakatwhitespace=true,
  tabsize=2
}

%\setlength\titlebox{5cm}

% You can expand the titlebox if you need extra space
% to show all the authors. Please do not make the titlebox
% smaller than 5cm (the original size); we will check this
% in the camera-ready version and ask you to change it back.


\title{CISC7021 - Applied Natural Language Processing - Assignment 2}

\author{Zhang Huakang\\
M-C3-5095-0
}

\date{}

\begin{document}
\maketitle

\section{Environment Setup}
In this project, four software packages are used: \textbf{Boost}, which is used for providing C++ libraries that are useful for tasks such as linear algebra, multithreading; \textbf{cmph}, which is a C library for minimal perfect hashing; \textbf{mosesdecoder}, which is a statistical machine translation system, and \textbf{mgiza}, which is a multi-threaded implementation of the word alignment tool GIZA++. 

The installation is done on a docker container with Ubuntu 14.04. This \textit{Dockerfile} is published on \href{https://github.com/BoxMars/NLP-HW/blob/main/Assignment2/Dockerfile}{Github}.

Since the unfamiliarity of the software packages, I spent a lot of time on the installation.

Because the tokenization of Chinese is different from English, I use \textit{ansjTokenizer} to tokenize Chinese, which is implemented by Java. \textit{openjdk:8-jdk} image is used to run the Java program.

\section{Tokenize}
\begin{lstlisting}[caption=Chinese Tokenization]
openjdk:8-jdk \
java -jar /mnt/ansjTokenizer.jar \
/mnt/train.tags.zh-en.zh \
/mnt/train.token.zh 
\end{lstlisting}
\begin{lstlisting}[caption=English Tokenization]
$MOSES_TOKEN/tokenizer.perl\
-l en \
-threads 4 \
< /mnt/train.tags.zh-en.en \
> /mnt/train.token.en 
\end{lstlisting}
\begin{lstlisting}[caption=Reduce Parallel Corpus]
$MOSES_TRAINING$/clean-corpus-n.perl\
/mnt/train.token \
en \
zh \
/mnt/train.token.clean.50 \
1 \
50 \
-lowercase 1
\end{lstlisting}
By the code in \textit{Listing 3}, we limit the length of the parallel corpus text to no more than fifty words, and convert the English to all lowercase.

Such pre-processing is also done for the test and dev data.

\section{3-gram Language Model}
\textit{LMPLZ} is used to train the language model, which is estimates language models with Modified Kneser-Ney smoothing and no pruning. The command is shown in \textit{Listing 4}.

\begin{lstlisting}[caption={Build Language Model}]
$MOSE_BIN/lmplz \
-o 3 \
-S 50% \
-T /mnt/tmp \
--text /mnt/train.token.clean.50.zh \
--arpa /mnt/train.token.clean.50.lm.zh \
--discount_fallback

$MOSE_BIN/lmplz \
-o 3 \
-S 50% \
-T /mnt/tmp \
--text /mnt/train.token.clean.50.en \
--arpa /mnt/train.token.clean.50.lm.en \
--discount_fallback
\end{lstlisting}
\subsection{Build Binary Language Model}
The binary language model is used to save memory and speed up the decoding process. The command is shown in \textit{Listing 5}.
\begin{lstlisting}[caption={Build Binary Language Model}]
$MOSES_BIN/build_binary \
/mnt/train.token.clean.50.lm.en \
/mnt/train.token.clean.50.blm.en

$MOSES_BIN/build_binary \
/mnt/train.token.clean.50.lm.zh \
/mnt/train.token.clean.50.blm.zh
\end{lstlisting}

\subsection{Test Language Model}
\begin{lstlisting}[caption=Test Language Model]
echo "is this an English sentence ?" | \
$MOSES_BIN/query \
/mnt/train.token.clean.50.blm.en
\end{lstlisting}
\begin{lstlisting}[caption=Query Output]
is=18 2 -2.6965108      this=45 3 -0.84844      an=281 3 -2.317651      English=0 1 -6.3741117     sentence=6235 1 -4.3026004      ?=54 2 -2.2268012       </s>=2 3 -0.21853548       Total: -18.984652 OOV: 1
Perplexity including OOVs:      515.3390835409365
Perplexity excluding OOVs:      126.40278774929547
OOVs:   1
Tokens: 7
Name:query      VmPeak:62996 kB VmRSS:3864 kB   RSSMax:47872 kB user:0.004339   sys:0.008679       CPU:0.013018    real:0.0113772
\end{lstlisting}

\section{Training}

\end{document}